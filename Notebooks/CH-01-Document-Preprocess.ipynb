{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Challenge 1: Explore Document Parsing and Chunking**\n",
        "\n",
        "Chunking helps limit the amount of information we pass into the model. The information that we will pass through are the most relevant chunks from the overall data. There are many considerations that come into play when chunking. For example, you need to figure out the best chunk size. If the chunks are too small, you may lose important context. If the chunks are too big, it may contain unnecessary information.\n",
        "\n",
        "## Let start the challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1716771639885
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment below line if running the notebook in Azure AI studio or Azure ML studio\n",
        "#%pip install -r requirements.txt\n",
        "\n",
        "import os\n",
        "import re\n",
        "import html\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "from azure.storage.blob import generate_blob_sas, BlobSasPermissions, BlobServiceClient\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1716771650757
        }
      },
      "outputs": [],
      "source": [
        "#Configure environment variables  \n",
        "load_dotenv(find_dotenv('credential.env'), override=True)\n",
        "\n",
        "#Azure storage account credentials\n",
        "connection_string = os.environ['AZURE_BLOB_STORAGE_CONNECTION_STRING']\n",
        "account_name =  os.environ['AZURE_BLOB_STORAGE_ACCOUNT_NAME']\n",
        "account_key =  os.environ['AZURE_BLOB_STORAGE_KEY']\n",
        "container_name =  os.environ['AZURE_BLOB_CONTAINER_NAME']\n",
        "\n",
        "#Azure Document Intelligence credentials\n",
        "endpoint = os.environ['AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT']\n",
        "key= os.environ['AZURE_DOCUMENT_INTELLIGENCE_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Declare useful method\n",
        "def check_and_create_folder(folder_name):\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "        print(f\"The folder '{folder_name}' has been created.\")\n",
        "    else:\n",
        "        print(f\"The folder '{folder_name}' already exists.\")\n",
        "\n",
        "def print_error_message(message, prefix_message='Error: '):\n",
        "    print(f\"\\033[1;31m{prefix_message}\\033[0m{message}\")\n",
        "\n",
        "def print_warning_message(message, prefix_message='Warning: '):\n",
        "    print(f\"\\033[1;33m{prefix_message}\\033[0m{message}\")\n",
        "    \n",
        "def print_success_message(message, prefix_message='Success: '):\n",
        "    print(f\"\\033[1;32m{prefix_message}\\033[0m{message}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Step1] Upload documents to Azure stroage account and generate SAS URL\n",
        "\n",
        "The following functions use Azure storage account SDK to upload and generate shared access signature (SAS) URL to be used as an input to Azure document intelligence. For security best practice, SAS will only valid for 1 hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1716771654160
        }
      },
      "outputs": [],
      "source": [
        "def upload_pdf_to_blobs():\n",
        "    # Blob connection\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "    blob_container = blob_service_client.get_container_client(container_name)\n",
        "    if not blob_container.exists():\n",
        "        blob_container.create_container()\n",
        "    \n",
        "    # Upload pdf file in pdf folder to blobs\n",
        "    file_names = []\n",
        "    for file in Path().glob(\"pdf_document/*.pdf\"):\n",
        "        blob_container.upload_blob(file.name, file.read_bytes(), overwrite=True)\n",
        "        file_names.append(file.name)\n",
        "    return file_names\n",
        "\n",
        "def get_blob_sas(account_name, account_key, container_name, blob_name):\n",
        "    sas_blob = generate_blob_sas(account_name=account_name, \n",
        "                                container_name=container_name,\n",
        "                                blob_name=blob_name,\n",
        "                                account_key=account_key,\n",
        "                                permission=BlobSasPermissions(read=True),\n",
        "                                expiry=datetime.now(ZoneInfo('UTC')) + timedelta(hours=1))\n",
        "    return sas_blob\n",
        "\n",
        "def get_pdf_file_names_from_blob(container_name, connection_string):\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "    container_client = blob_service_client.get_container_client(container_name)\n",
        "    \n",
        "    file_names = []\n",
        "    blob_list = container_client.list_blobs()\n",
        "\n",
        "    for blob in blob_list:\n",
        "        if blob.name.endswith('.pdf'):\n",
        "            file_names.append(blob.name)\n",
        "    \n",
        "    return file_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716771707917
        }
      },
      "outputs": [],
      "source": [
        "pdf_names = []\n",
        "url_list = []\n",
        "\n",
        "print_warning_message(\"Upload documents to Azure storage account and generate SAS URL\", \">>>[Step1] \")\n",
        "\n",
        "#Upload PDF file from local folder \"pdf_document\" folder to Azure storage account\n",
        "local_pdf_file_names = upload_pdf_to_blobs()\n",
        "for pdf in local_pdf_file_names:\n",
        "    print_success_message(f\"{pdf} PDF files uploaded to blob storage {container_name}\")\n",
        "\n",
        "#Get a list of file name in specific blob container in Azure storage account\n",
        "pdf_names = get_pdf_file_names_from_blob(container_name, connection_string)\n",
        "\n",
        "#Generate SAS URLs of pdf files in Azure storage account.\n",
        "for pdf in pdf_names:\n",
        "    blob_sas = get_blob_sas(account_name, account_key, container_name, pdf)\n",
        "    url = 'https://'+account_name+'.blob.core.windows.net/'+container_name+'/'+pdf+'?'+blob_sas\n",
        "    url_list.append(url)\n",
        "\n",
        "#Print URL list with SAS of each PDF document in blob storage\n",
        "print_success_message(\"Generate SAS URLs for all files in \"+container_name+\" blob container\")\n",
        "url_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Step2] Parsing PDF document using Azure Document Intelligence. \n",
        "\n",
        "There are many prebuit model which we can use to parse different format of document. We will explore **prebuilt-read** model in this challenge. It supports various file types including PDF, Microsoft office (DOCX, PPTX, XLSX), HTML and images (JPG, PNG, BMP, TIFF, HEIF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716771728842
        }
      },
      "outputs": [],
      "source": [
        "model_id = 'prebuilt-read'\n",
        "api_version = '2023-10-31-preview'\n",
        "\n",
        "# Set the local folder name for document intelligence output\n",
        "folder_name = \"document_intelligence_output\"\n",
        "\n",
        "# Check if the folder exists\n",
        "check_and_create_folder(\"document_intelligence_output\")\n",
        "\n",
        "print_warning_message(\"Parsing PDF document using Azure Document Intelligence using \" + model_id + \" model. Please wait...\", \">>>[Step2] \")\n",
        "\n",
        "# for index, url in enumerate(url_list):\n",
        "for index, (name, url) in enumerate(zip(pdf_names, url_list)):\n",
        "    print(f\"Processing document: {name}\")\n",
        "    payload = {\n",
        "        \"urlSource\": url\n",
        "    }\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': key\n",
        "    }\n",
        "\n",
        "    #Send a request to document intelligence endpoint with API keys and version\n",
        "    response = requests.post(url=f\"{endpoint}documentintelligence/documentModels/{model_id}:analyze?api-version={api_version}\", headers=headers, json=payload)\n",
        "    \n",
        "    if not response.ok:\n",
        "        response.raise_for_status()\n",
        "    \n",
        "    time.sleep(5)\n",
        "\n",
        "    #Add delay when processing each file to allow time for SDK finish the previous document processing.\n",
        "    for sleep_time in [20,40,60,120,240,960]:\n",
        "    # while (True):\n",
        "        response_2 = requests.get(response.headers['Operation-Location'], headers=headers)\n",
        "        rst = response_2.json()\n",
        "\n",
        "        if rst['status'] == 'succeeded':\n",
        "            Path(f\"document_intelligence_output/{index}.json\").write_text(json.dumps(rst))\n",
        "            print_success_message(\"\", \"Success!\")\n",
        "            break\n",
        "        else:\n",
        "            time.sleep(sleep_time)\n",
        "    else:\n",
        "        print_error_message(\"Failed time out\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [Check Point] Take a moment to view the output of document parsing under folder \"document_intelligence_output\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Step3] Break down JSON output to mulitple 1,000 characters document chunks\n",
        "\n",
        "There are multiple chunking techniques we can use. In this challenge, we will explore chunking by length of 1,000 characters with 10 percents overlaping. You may need to explore the right chunking techniques that work best with your data. Other chunking techniques you may consider such as \n",
        "- Chunking by length\n",
        "- Chunking by page\n",
        "- Chunking by semantic\n",
        "- Chunking by sentences\n",
        "- Chunking recursively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1716771732760
        }
      },
      "outputs": [],
      "source": [
        "MAX_SECTION_LENGTH = 1000\n",
        "SENTENCE_SEARCH_LIMIT = 100\n",
        "SECTION_OVERLAP = 100\n",
        "\n",
        "#Convert table to HTML format\n",
        "def table_to_html(table):\n",
        "    table_html = \"<table>\"\n",
        "    rows = [sorted([cell for cell in table.cells if cell.row_index == i], key=lambda cell: cell.column_index) for i in range(table.row_count)]\n",
        "    for row_cells in rows:\n",
        "        table_html += \"<tr>\"\n",
        "        for cell in row_cells:\n",
        "            tag = \"th\" if (cell.kind == \"columnHeader\" or cell.kind == \"rowHeader\") else \"td\"\n",
        "            cell_spans = \"\"\n",
        "            if cell.column_span > 1: cell_spans += f\" colSpan={cell.column_span}\"\n",
        "            if cell.row_span > 1: cell_spans += f\" rowSpan={cell.row_span}\"\n",
        "            table_html += f\"<{tag}{cell_spans}>{html.escape(cell['content'])}</{tag}>\"\n",
        "        table_html +=\"</tr>\"\n",
        "    table_html += \"</table>\"\n",
        "    return table_html\n",
        "\n",
        "def get_document_text(form_recognizer_results):\n",
        "        offset = 0\n",
        "        page_map = []\n",
        "        for page_num, page in enumerate(form_recognizer_results['pages']):\n",
        "            try:\n",
        "                tables_on_page = [table for table in form_recognizer_results['tables'] if table.bounding_regions[0].page_number == page_num + 1]    \n",
        "            except:\n",
        "                tables_on_page = []\n",
        "\n",
        "            # mark all positions of the table spans in the page\n",
        "            page_offset = page['spans'][0]['offset']\n",
        "            page_length = page['spans'][0]['length']\n",
        "            table_chars = [-1]*page_length\n",
        "            for table_id, table in enumerate(tables_on_page):\n",
        "                for span in table['spans']:\n",
        "                    # replace all table spans with \"table_id\" in table_chars array\n",
        "                    for i in range(span['length']):\n",
        "                        idx = span['offset'] - page_offset + i\n",
        "                        if idx >=0 and idx < page_length:\n",
        "                            table_chars[idx] = table_id\n",
        "\n",
        "            # build page text by replacing charcters in table spans with table html\n",
        "            page_text = \"\"\n",
        "            added_tables = set()\n",
        "            for idx, table_id in enumerate(table_chars):\n",
        "                if table_id == -1:\n",
        "                    page_text += form_recognizer_results['content'][page_offset + idx]\n",
        "                elif not table_id in added_tables:\n",
        "                    page_text += table_to_html(tables_on_page[table_id])\n",
        "                    added_tables.add(table_id)\n",
        "\n",
        "            page_text += \" \"\n",
        "            page_map.append((page_num, offset, page_text))\n",
        "            offset += len(page_text)\n",
        "\n",
        "        return page_map\n",
        "\n",
        "#Break down text according to defined length with overlapping in each chunk\n",
        "def split_text(page_map):\n",
        "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
        "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]\n",
        "     \n",
        "\n",
        "    def find_page(offset):\n",
        "        l = len(page_map)\n",
        "        for i in range(l - 1):\n",
        "            if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
        "                return i\n",
        "        return l - 1\n",
        "\n",
        "    all_text = \"\".join(p[2] for p in page_map)\n",
        "    length = len(all_text)\n",
        "    start = 0\n",
        "    end = length\n",
        "    while start + SECTION_OVERLAP < length:\n",
        "        last_word = -1\n",
        "        end = start + MAX_SECTION_LENGTH\n",
        "\n",
        "        if end > length:\n",
        "            end = length\n",
        "        else:\n",
        "            # Try to find the end of the sentence\n",
        "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
        "                if all_text[end] in WORDS_BREAKS:\n",
        "                    last_word = end\n",
        "                end += 1\n",
        "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
        "                end = last_word # Fall back to at least keeping a whole word\n",
        "        if end < length:\n",
        "            end += 1\n",
        "\n",
        "        # Try to find the start of the sentence or at least a whole word boundary\n",
        "        last_word = -1\n",
        "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
        "            if all_text[start] in WORDS_BREAKS:\n",
        "                last_word = start\n",
        "            start -= 1\n",
        "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
        "            start = last_word\n",
        "        if start > 0:\n",
        "            start += 1\n",
        "\n",
        "        section_text = all_text[start:end]\n",
        "        yield (section_text, find_page(start))\n",
        "\n",
        "        last_table_start = section_text.rfind(\"<table\")\n",
        "        if (last_table_start > 2 * SENTENCE_SEARCH_LIMIT and last_table_start > section_text.rfind(\"</table\")):\n",
        "            # If the section ends with an unclosed table, we need to start the next section with the table.\n",
        "            # If table starts inside SENTENCE_SEARCH_LIMIT, we ignore it, as that will cause an infinite loop for tables longer than MAX_SECTION_LENGTH\n",
        "            # If last table starts inside SECTION_OVERLAP, keep overlapping\n",
        "            \n",
        "            start = min(end - SECTION_OVERLAP, start + last_table_start)\n",
        "        else:\n",
        "            start = end - SECTION_OVERLAP\n",
        "        \n",
        "    if start + SECTION_OVERLAP < end:\n",
        "        yield (all_text[start:end], find_page(start))\n",
        "\n",
        "#Extend the PDF file name with page number\n",
        "def blob_name_from_file_page(filename, page = 0):\n",
        "    if os.path.splitext(filename)[1].lower() == \".pdf\":\n",
        "        return os.path.splitext(os.path.basename(filename))[0] + f\"-{page}\" + \".pdf\"\n",
        "    else:\n",
        "        return os.path.basename(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716771747561
        }
      },
      "outputs": [],
      "source": [
        "print_warning_message(\"Break down JSON output to mulitple \"+str(MAX_SECTION_LENGTH)+\" characters document chunks\", \">>>[Step3] \")\n",
        "\n",
        "# Check if the folder exists\n",
        "check_and_create_folder(\"chunked_document\")\n",
        "\n",
        "for index, file_name in enumerate(pdf_names):\n",
        "    try:\n",
        "        with open(f'document_intelligence_output/{str(index)}.json') as JSON:\n",
        "            raw_json = json.load(JSON)\n",
        "    except:\n",
        "        continue\n",
        "    \n",
        "    page_map = get_document_text(raw_json['analyzeResult'])\n",
        "    \n",
        "    #Format the JSON chunked to match with Azure AI search index profile\n",
        "    for i, (section, pagenum) in enumerate(split_text(page_map)):\n",
        "        try:\n",
        "            doc = {\n",
        "                \"id\": re.sub(\"[^0-9a-zA-Z_-]\",\"_\",f\"{file_name}-{i}\"),\n",
        "                \"content\": section,   \n",
        "                #\"state\":  file_name.split('/')[1] if file_name.split('/')[0] == 'States' else '', \n",
        "                \"category\": 'Not Available',\n",
        "                \"sourcepage\": blob_name_from_file_page(file_name, pagenum),\n",
        "                \"sourcefile\": file_name\n",
        "            }\n",
        "            with open(f'chunked_document/{doc[\"id\"]}.json', 'w') as file:\n",
        "                json.dump(doc, file)\n",
        "            print_success_message(\"Chunked document #\" + str(i) + \" is created in 'chunk_document' folder\")\n",
        "        except Exception as error:\n",
        "            print_error_message(error, f\"Error on '{file_name}' | chunk#{i}: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### [Check Point] Review chunked document JSON output under \"chucked_document\" folder"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
