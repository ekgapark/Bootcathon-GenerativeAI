{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Challenge 3: Retrieve search output and interact with Azure OpenAI**\n",
        "\n",
        "### Azure AI Search Method\n",
        "\n",
        "Azure AI Search service enhances search capabilities by offering various methods such as pure vector search, hybrid search, and semantic hybrid search.\n",
        "\n",
        "- **Pure Vector Search:** Utilizes vector representations of documents to perform searches based on semantic similarity. This method is ideal for applications requiring high precision in finding relevant passages or documents through semantic matching.\n",
        "    - Use case: Matching image content with text content, cross-lingual searches, or finding similar documents.\n",
        "- **Hybrid Search**: Combines traditional keyword search with vector search, applying a fusion step to select the best results from each technique. It’s particularly effective for general search use cases where both relevance and keyword matching are important.\n",
        "    - Use case: Retrieval-augmented generation applications, where both the retrieval of relevant documents and the ranking based on semantic relevance are key.\n",
        "- **Semantic Hybrid Search**: Integrates semantic ranking with hybrid search to further refine the relevance of search results. This approach is most beneficial for Generative AI scenarios, where precise retrieval of information is crucial for generating accurate responses.\n",
        "    - Use case: Enhancing chat-style and copilot applications by ensuring the top search results are the most semantically relevant to the query.\n",
        "\n",
        "## Let start the challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1716804388482
        }
      },
      "outputs": [],
      "source": [
        "# Import required libraries  \n",
        "import os\n",
        "import textwrap\n",
        "from openai import AzureOpenAI, DefaultHttpxClient\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from azure.core.credentials import AzureKeyCredential  \n",
        "from azure.search.documents import SearchClient  \n",
        "from azure.search.documents.models import Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1716804390784
        }
      },
      "outputs": [],
      "source": [
        "# Configure environment variables  \n",
        "load_dotenv(find_dotenv('credential.env'), override=True)\n",
        "\n",
        "# Azure AI Search\n",
        "service_endpoint = os.environ['AZURE_AI_SEARCH_ENDPOINT']\n",
        "key = os.environ['AZURE_AI_SEARCH_KEY']\n",
        "index_name = os.environ['AZURE_AI_SEARCH_INDEX_NAME']\n",
        "credential = AzureKeyCredential(key)\n",
        "\n",
        "#Azure OpenAI\n",
        "client = AzureOpenAI(\n",
        "  api_key = os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
        "  azure_endpoint = os.environ['AZURE_OPENAI_API_ENDPOINT'],\n",
        "  api_version = os.environ['AZURE_OPENAI_API_VERSION'],\n",
        "  http_client=DefaultHttpxClient(verify=False)\n",
        ")\n",
        "embedding_model = os.environ['EMBEDDING_MODEL_NAME']\n",
        "gpt35turbo_model = os.environ['GPT35TURBO_MODEL_NAME']\n",
        "gpt4turbo_model = os.environ['GPT4TURBO_MODEL_NAME']\n",
        "gpt4o_model = os.environ['GPT4O_MODEL_NAME']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Declare useful method\n",
        "def check_and_create_folder(folder_name):\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "        print(f\"The folder '{folder_name}' has been created.\")\n",
        "    else:\n",
        "        print(f\"The folder '{folder_name}' already exists.\")\n",
        "\n",
        "def print_error_message(message, prefix_message='Error: '):\n",
        "    print(f\"\\033[1;31m{prefix_message}\\033[0m{message}\")\n",
        "\n",
        "def print_warning_message(message, prefix_message='Warning: '):\n",
        "    print(f\"\\033[1;33m{prefix_message}\\033[0m{message}\")\n",
        "    \n",
        "def print_success_message(message, prefix_message='Success: '):\n",
        "    print(f\"\\033[1;32m{prefix_message}\\033[0m{message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1716807286020
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(text):\n",
        "    response = client.embeddings.create(input=text, model=embedding_model)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "\n",
        "def get_chat_completion(messages, model):\n",
        "    print(f'Generating answer using {model} LLM model')\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def format_retrieved_documents(data_list):\n",
        "    retrieved_documents = \"\"\n",
        "    for item in data_list:\n",
        "        # Extract the 'content' and 'sourcepage' values\n",
        "        content = item.get('content', '')\n",
        "        sourcepage = item.get('sourcepage', '')\n",
        "        retrieved_documents += \"sourcepage: \" + sourcepage + \", content: \" + content + \"\\n\"\n",
        "    return retrieved_documents"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform a vector similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716782821322
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Pure Vector Search\n",
        "query = \"What is GenAI?\"  \n",
        "\n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
        "\n",
        "# Embedding \"query\" before searching\n",
        "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")\n",
        "\n",
        "# Only search using vector content to query agaist \"contentVector\" field\n",
        "results = search_client.search(  \n",
        "    search_text=None,  \n",
        "    vectors=[vector],\n",
        "    select=[\"content\", \"sourcepage\"],\n",
        ")\n",
        "\n",
        "result_list = []\n",
        "for result in results:  \n",
        "    print_warning_message(f\"{result['sourcepage']}\", \"Source Page: \")\n",
        "    print_warning_message(f\"{result['@search.score']}\", \"Search Score: \")\n",
        "    print_warning_message(f\"{textwrap.shorten(result['content'], width=100, placeholder=\"...\")}\\n\", \"Content: \")\n",
        "    result_list.append(result)\n",
        "\n",
        "print_success_message(\"\", \"-----AI Search JSON Output [Pure Vector Search]-----\")\n",
        "result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716777232494
        }
      },
      "outputs": [],
      "source": [
        "# Pure Vector Search multi-lingual \n",
        "query = \"GenAI คืออะไร\"  \n",
        "  \n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
        "\n",
        "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")\n",
        "\n",
        "results = search_client.search(  \n",
        "    search_text=None,  \n",
        "    vectors=[vector],\n",
        "    select=[\"content\", \"sourcepage\"],\n",
        ")  \n",
        "\n",
        "result_list = []\n",
        "for result in results:  \n",
        "    print_warning_message(f\"{result['sourcepage']}\", \"Source Page: \")\n",
        "    print_warning_message(f\"{result['@search.score']}\", \"Search Score: \")\n",
        "    print_warning_message(f\"{textwrap.shorten(result['content'], width=100, placeholder=\"...\")}\\n\", \"Content: \")\n",
        "    result_list.append(result)\n",
        "    \n",
        "\n",
        "print_success_message(\"\", \"-----AI Search JSON Output [Pure Vector Search]-----\")\n",
        "result_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform a Hybrid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716807530885
        }
      },
      "outputs": [],
      "source": [
        "# Hybrid Search\n",
        "query = \"What is GenAI?\"  \n",
        "  \n",
        "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
        "\n",
        "# Embedding \"query\" before searching\n",
        "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")\n",
        "\n",
        "# Search using as it \"query\" as a semantic search together with vector query\n",
        "results = search_client.search(  \n",
        "    search_text=query,  \n",
        "    vectors=[vector],\n",
        "    select=[\"content\", \"sourcepage\"],\n",
        "    top=3\n",
        ")  \n",
        "\n",
        "result_list = []\n",
        "for result in results:  \n",
        "    print_warning_message(f\"{result['sourcepage']}\", \"Source Page: \")\n",
        "    print_warning_message(f\"{result['@search.score']}\", \"Search Score: \")\n",
        "    print_warning_message(f\"{textwrap.shorten(result['content'], width=100, placeholder=\"...\")}\\n\", \"Content: \")\n",
        "    result_list.append(result)\n",
        "\n",
        "print_success_message(\"\", \"-----AI Search JSON Output [Hybrid Search]-----\")\n",
        "result_list"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform a Semantic Hybrid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716807484505
        }
      },
      "outputs": [],
      "source": [
        "# Semantic Hybrid Search\n",
        "query = \"what is genai?\"\n",
        "\n",
        "search_client = SearchClient(\n",
        "    service_endpoint, index_name, credential=credential)\n",
        "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")\n",
        "\n",
        "results = search_client.search(\n",
        "    search_text=query,\n",
        "    vectors=[vector],\n",
        "    select=[\"content\", \"sourcepage\"],\n",
        "    query_type=\"semantic\", query_language=\"en-us\", semantic_configuration_name='my-semantic-config', query_caption=\"extractive\", query_answer=\"extractive\",\n",
        "    top=3\n",
        ")\n",
        "\n",
        "result_list = []\n",
        "# Retrieve top k relevance chunked document from Azure AI Search\n",
        "for result in results:\n",
        "    print_warning_message(f\"{result['sourcepage']}\", \"Source Page: \")\n",
        "    print_warning_message(f\"{result['@search.score']}\", \"Search Score: \")\n",
        "    print_warning_message(f\"{textwrap.shorten(result['content'], width=100, placeholder=\"...\")}\", \"Content: \")\n",
        "    print_warning_message(f\"{result['@search.reranker_score']}\", \"Reranker Score: \")\n",
        "    result_list.append(result)\n",
        "\n",
        "    captions = result[\"@search.captions\"]\n",
        "    if captions:\n",
        "        caption = captions[0]\n",
        "        if caption.highlights:\n",
        "            print_warning_message(f\"{caption.highlights}\\n\", \"Caption: \")\n",
        "        else:\n",
        "            print_warning_message(f\"{caption.text}\\n\", \"Caption: \")\n",
        "\n",
        "# Generate semantic answers based on top k relevance chunked documents\n",
        "semantic_answers = results.get_answers()\n",
        "for answer in semantic_answers:\n",
        "    if answer.highlights:\n",
        "        print_warning_message(f\"{answer.highlights}\", \"Semantic Answer: \")\n",
        "    else:\n",
        "        print_warning_message(f\"{answer.text}\", \"Semantic Answer: \")\n",
        "    print_warning_message(f\"{answer.score}\\n\", \"Semantic Answer Score: \")\n",
        "\n",
        "print_success_message(\"\", \"-----AI Search JSON Output [Semantic Hybrid Search with reranking]-----\")\n",
        "result_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define system prompt for large language model\n",
        "\n",
        "In the context of Azure OpenAI, the roles of the system, user, and assistant are crucial for the effective functioning of the AI models, particularly in chat-based interactions. Here’s a breakdown of each role and the importance of the system message:\n",
        "\n",
        "**System:** This is a predefined message that sets the stage for the AI’s behavior. It primes the model with context, instructions, or other relevant information for the specific use case. The system message can describe the assistant’s personality, outline what the model should and shouldn’t answer, and define the format of the model’s responses. It’s a critical component because it helps ensure that the AI operates within the desired parameters and provides responses that are aligned with the user’s expectations.\n",
        "\n",
        "**User:** The user interacts with the AI model by providing input or asking questions. The user’s messages are the prompts that the AI responds to. The quality and clarity of the user’s input can significantly influence the accuracy and relevance of the AI’s responses.\n",
        "\n",
        "**Assistant:** The assistant is the AI itself, which processes the user’s input and the context provided by the system message to generate helpful, accurate, and contextually appropriate responses. The assistant’s role is to assist users by providing information, answering questions, and engaging in conversation within the guidelines set by the system message.\n",
        "\n",
        "The importance of the system message lies in its ability to guide the AI system’s behavior and improve its performance. It defines the AI’s capabilities and limitations, the expected output format, and additional safety and behavioral guardrails. By crafting an effective system message, developers can increase the accuracy and grounding of the responses generated by the AI, making it a more reliable and efficient tool for users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716807539497
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"\n",
        "## On your role\n",
        "- You're an AI assistance to help answer questions based on retrieved documents.\n",
        "## Retrieved documents\n",
        "\"\"\" + format_retrieved_documents(result_list)\n",
        "\n",
        "print(system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Call Azure OpenAI API to generate the answer based retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716808000088
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define messages format to interact with LLM model\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "# GPT Model Option: gpt35turbo_model, gpt4turbo_model and gpt4o_model\n",
        "response = get_chat_completion(messages, gpt4o_model)\n",
        "print(\"assistant: \"+response)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
